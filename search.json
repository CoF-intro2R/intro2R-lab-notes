[
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "Week 1",
    "section": "",
    "text": "Instructor: Dusty Gannon\n\nMS Statistics, OSU\nPhD Botany and Plant Pathology, OSU\n\n\n\n\nStatistics\nPollination ecology\nPopulation biology and ecology\nR\nStan/JAGS\nC++\nLaTeX\npython(ish)"
  },
  {
    "objectID": "day1.html#expertise",
    "href": "day1.html#expertise",
    "title": "Week 1",
    "section": "",
    "text": "Statistics\nPollination ecology\nPopulation biology and ecology\nR\nStan/JAGS\nC++\nLaTeX\npython(ish)"
  },
  {
    "objectID": "day1.html#why-r",
    "href": "day1.html#why-r",
    "title": "Week 1",
    "section": "Why R?",
    "text": "Why R?\n\nStatistical analysis\nRelatively easy to learn\nExcellent tools for cleaning, formatting, and analyzing tabular data"
  },
  {
    "objectID": "day1.html#taking-project-organization-to-the-next-level",
    "href": "day1.html#taking-project-organization-to-the-next-level",
    "title": "Week 1",
    "section": "Taking project organization to the next level",
    "text": "Taking project organization to the next level\nCollaboration is an important aspect of modern science. How do we work collaboratively on code? Let‚Äôs see the importance of some of the tools you will be introduced to with an example.\n\n\n\n\n\n\nYour turn\n\n\n\n\nOpen your system‚Äôs file manager (Finder for Mac, File Explorer for Windows). Inside your Documents directory, create a new folder called CoF_intro2R. Note the use of - or _ in place of spaces.\nInside this folder, create another called Lab and yet another inside that called Day1.\nInside of the Day1 folder, create a new, blank text file. Copy the following text into the file:\n\na,b,c\n1,2,3\n\nSave the file as eg.csv.\n\nWhat is the name of the .csv file you just created?\n\nOpen RStudio.\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nLocal name: eg.csv\nFull name: Something along the lines of N:\\Users\\&lt;username&gt;\\Documents\\CoF_intro2R\\Lab\\Day1\\eg.csv\n\n\n\n\n\nAbsolute vs.¬†relative paths\n\nRelative paths: Paths to a location that is relative to a specified location, such as the location of the current document or a project directory.\nAbsolution paths: Paths to files and locations relative to the root of the file system."
  },
  {
    "objectID": "day1.html#a-short-aside-navigating-rstudio",
    "href": "day1.html#a-short-aside-navigating-rstudio",
    "title": "Week 1",
    "section": "A short aside: Navigating RStudio",
    "text": "A short aside: Navigating RStudio\n\n4 Panes\n\nConsole\nEditor - Where you will edit R scripts and other documents\nEnvironment - Where you can see objects stored in memory\nFiles/Viewer - Where you can navigate the file system and see outputs of plots and other visuals.\n\nCustomizing the appearance (Very important üòâ)\n\nTools -&gt; Global options -&gt; Appearance\n\n\nOther important global options\n\nTools -&gt; Global options -&gt; General\n\nIn the Workspace and History settings, adjust the defaults to match the image below.\n\n\n\n\nIf working on a Windows machine, change your default terminal to PowerShell or some similar bash-like terminal by going to Tools -&gt; Global Options -&gt; Terminal and change the options that says New terminals open with‚Ä¶."
  },
  {
    "objectID": "day1.html#back-to-project-organization",
    "href": "day1.html#back-to-project-organization",
    "title": "Week 1",
    "section": "Back to project organization",
    "text": "Back to project organization\ntesting\n\n\n\n\n\n\nYour Turn\n\n\n\nIn the terminal tab of the console pane, type\npwd\nCompare the result to your neighbor‚Äôs result. Do you get the same results?\nExplore the file tree of your machine using the command line. The only necessary command is cd (for ‚Äòchange directory‚Äô).\ncd ../\nwill take you up one level (i.e., out one level from the nested structure of the file tree). To go further down or into a specific directory, use\ncd &lt;directory&gt;\nreplacing &lt;directory&gt; with the directory name.\nNote, that you can use the tab key to autocomplete the directory name. So start typing the directory name, then press tab to autocomplete the name.\nFeel free to get lost a bit. If you do,\ncd ~\nwill take you back home üè†.\nFinally, find your way to the Day1 directory you created above.\n\n\nOne of the things that trips people up when they first begin using programming languages to do data analysis is file path management. Good, consistent project organization combined with RProjects can relieve this headache substantially.\n\n\n\n\n\n\nYour Turn\n\n\n\nLet‚Äôs practice loading data into R in order to see how file paths can trip people up and break code in many circumstances.\n\nUse the pwd command in the terminal to see the path to your data file, eg.csv.\nOpen a new RScript file and type the following:\n\n( dat &lt;- read.csv(\"&lt;file-path&gt;\") )\nreplacing &lt;file-path&gt; with the path you got using pwd and appending /eg.csv onto the end of the path.\n\nRun the line using CMD + ENTER, or the ‚ÄúRun‚Äù button at the top of the script pane.\nBelow this line, add the line from your neighbor‚Äôs script. What happens?\nDelete or comment your neighbor‚Äôs line out of your script and save the file to the Day1 directory. Name the file file_paths.R.\n\n\n\n\nA better way\n\n\n\n\n\n\nYour Turn\n\n\n\n\nIn the upper right of RStudio, find the Project dropdown menu. Select Open Project and navigate to your new CoF_intro2R/Labs directory. Open it as an R project.\nOpen the file_paths.R script. Type getwd() (for get working directory). What do you notice?\nReplace your previous line with the relative file path to the data. What do you notice about the line in your script and your neighbor‚Äôs?\nAs an alternative to the simple relative path, you can use the here package for R. To do this, first install the here package using\n\ninstall.packages(\"here\")\nin the R console.\n\nAdd the following line to your script, replacing &lt;relative-path&gt; with the path to the eg.csv file from the root of the project directory.\n\nhere::here()\ndat2 &lt;- read.csv(here::here(\"&lt;relative-path&gt;\"))\nall.equal(dat, dat2)\nü§î Does the first command, here::here(), give the same result for you and your neighbor?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nUsing here::here() in the console will usually not give the same result for two people on two machines because their project directories are stored in different place (literally two different computers with different users).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe double-colon syntax, package::function(), as we are using with here::here() is a safer way of using functions from packages downloaded from the internet than loading the full library and then calling the function without prefacing with the package name. We will get into packages more later in the workshop, but for now, remember that we can use any function from a downloaded package using this syntax."
  },
  {
    "objectID": "day1.html#key-git-vocabulary",
    "href": "day1.html#key-git-vocabulary",
    "title": "Week 1",
    "section": "Key git vocabulary",
    "text": "Key git vocabulary\n\nNouns\n\nrepository/repo: Effectively a folder/directory\nremote: The version of the repo that is hosted on GitHub or some other web-based hosting platform\nlocal: The version of the repo that is stored on your personal computer\nstaging area: A list of files that should be ‚Äútracked‚Äù, or version controlled, using git.\ncommit: When used as a noun, a recorded change or set of changes to the repo.\nbranch1: A separate record of changes to the repo. There is usually a main branch that serves as the reference and feature branches that can be merged with main once the edits to the repo are complete. Branches are useful for collaborative repositories.\npull request2: A set of suggested changes to a repo that can be suggested by a collaborator or, if the repo is public, anyone on the internet. The suggestions can then be reviewed by the owner of the repo and merged or not.\n\n\n\nVerbs\n\nclone: To copy a repo from GitHub that you have admin or collaborator privelages on\nfork: To copy a repo from GitHub (or another host) that you do not have write priveleges to. The only way to contribute to a forked repo is through pull requests.\npull: To sync changes from the remote to your local repo\npush: To sync changes from your local repo to the remote\ncommit: When used as a verb, a set of change for which you want to create a time-stamp/record of the repo at that point in time.\ncommit message:"
  },
  {
    "objectID": "day1.html#a-simple-workflow",
    "href": "day1.html#a-simple-workflow",
    "title": "Week 1",
    "section": "A simple workflow",
    "text": "A simple workflow\n\n\n\n\n\ngraph LR\n    A[fa:fa-github Initial state] -.-&gt; D[fa:fa-github updated remote]\n    A -- clone to local --&gt; B(fa:fa-laptop initial local)\n    B -- commit changes --&gt; C(fa:fa-laptop updated local)\n    C -- push changes --&gt; D\n    \nclassDef remote fill:#ececec,stroke:#2d2926,color:#000000;\nclassDef local fill:#81a9ad,stroke:#2d2926,color:#000000;\nclass A,D remote\nclass B,C local\n\n\n\n\n\n\n\nMultiple computers\n\n\n\n\n\ngraph TD\n    A[fa:fa-github Initial state] -.-&gt; E\n    A -- clone to local 1 --&gt; B(fa:fa-laptop initial local 1)\n    A -- clone to local 2 --&gt; C(fa:fa-desktop initial local 2)\n    B -- commit changes --&gt; D(fa:fa-laptop update1 to local 1)\n    D -- push to remote --&gt; E[fa:fa-github update 1 to remote]\n    E -.-&gt; H\n    E -- pull to local 2 --&gt; F(fa:fa-desktop update 1 to local 2)\n    C -. fast forward .-&gt; F\n    F -- commit changes --&gt; G(fa:fa-desktop update 2 local 2)\n    G -- push to remote --&gt; H[fa:fa-github update 2 to remote]\n    \nclassDef remote fill:#ececec,stroke:#2d2926,color:#000000;\nclassDef local1 fill:#81a9ad,stroke:#2d2926,color:#000000;\nclassDef local2 fill:#537380,stroke:#2d2926,color:#FFFFFF;\nclass A,E,H remote\nclass B,D local1\nclass C,F,G local2"
  },
  {
    "objectID": "day1.html#using-git-and-github",
    "href": "day1.html#using-git-and-github",
    "title": "Week 1",
    "section": "Using git and GitHub",
    "text": "Using git and GitHub\nLet‚Äôs get some practice with the git workflow while also introducing how your homeworks will be assigned.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nCreate a Homework directory inside your CoF_intro2R directory.\nGo to the Canvas site for this class, Week 1 module. Click the link provided on the Homework 1 page. ‚ÄúAccept‚Äù the assignment. This will take you to GitHub (it may ask you to sign in) and a newly created template repository for you.\nClone the repo to your local computer using one of the two methods below. Clone the repo into the Homework directory.\nFollow the homework instructions for steps 1-3.\nPush your changes up to the remote using one of the three methods below.\n\n\n\n\nUsing git from RStudio\nThe most common git commands have been baked into the RStudio GUI. These include adding files to the staging area, committing and writing commit messages, pushing, and pulling.\n\nIf you are working on a Windows machine, I have found that you often need to specify where your git.exe file is stored and point RStudio to that location. To do so, go to Tools -&gt; Global Options -&gt; Git/SVN, then either browse or supply the filepath to the git executable. For example, for those that downloaded git through GitHub Desktop, the git.exe is located in C:/Users/&lt;username&gt;/AppData/Local/GitHubDesktop/app-&lt;version&gt;/resources/app/git/cmd/git.exe.\n\n\nCloning\nTo clone a repo to your local computer, go to File -&gt; New Project‚Ä¶ -&gt; Version Control -&gt; Git.\nNext, go to your GitHub repo and click the green ‚ÄúCode‚Äù button. Copy the HTTPs URL and paste it into the dialogue box from RStudio. You can then save it to a specific location. This will open the repo as an RProject.\n\n\n\n\n\n\nNote\n\n\n\nIf you set up your ssh keys before the course started (extra credit on homework 0), then be sure to clone your project from GitHub using the ssh protocol, not HTTPs.\n\n\n\n\nPulling\nExcept for immediately after the initial cloning step when you are copying the remote repo to your local machine, the first thing you should do upon opening a repo locally is pull. This merges any new edits that are saved on the remote that you don‚Äôt have locally. This is easily accomplished in RStudio using the blue down arrow ‚¨áÔ∏è on the Git pane.\n\n\nCommitting\nIn the upper right panel of RStudio, you will now see a git tab.\n\nAfter making changes to files, click the green checkmark ‚úîÔ∏è icon that says ‚Äúcommit‚Äù when you hover over it with your mouse. This will bring up a new dialogue box.\nCheck the box next to the file you changed and to which you want to commit the changes. You can see the changes made in the panel at the bottom.\nWrite a commit message in the box that describes the changes made or the state the project is in. Ideally, you should be able to look at commit messages and use them to revert the project back to previous states if necessary.\nPress ‚Äúcommit‚Äù.\n\n\n\nPushing\nOnce you are ready to merge your local changes with the remote repo, press the green up arrow ‚¨ÜÔ∏è in the Git pane of RStudio. The remote repo will be updated with the changes you made during the working session.\n\n\n\nUsing GitHub Desktop\nGitHub Desktop gives the user a GUI with which to interact. To clone a repository to your local computer using GitHub Desktop, open GitHub Desktop and sign into your GitHub account.\n\nCloning a repo\nOnce signed into GitHub from GitHub Desktop, click Clone a Repository from the Internet.... GitHub Desktop should already be linked to your GitHub account, so you just need to search for the repo you want. Clone the repo to the local location you want using the Choose button to navigate to the local path. Then click Clone.\n\n\nPulling\n\nSelect the repo you want to work on. In the top-left corner of GitHub Desktop, you will see a drop-down menu. Click this and select the repository you want to pull changes into.\nIn the top-center of the GitHub Desktop window, check that you are on the correct branch (e.g., main or develop). You can switch branches using the drop-down menu if needed3.\nOn the toolbar at the top, click the Fetch origin button. This will check for any updates or changes made to the repository on GitHub.\nOnce GitHub Desktop checks for updates, the Fetch origin button will change to Pull origin if there are any updates. Click the Pull origin button to download and apply the changes from the remote repository to your local copy.\n\n\n\nCommitting changes\n\nOnce you‚Äôve made changes, open GitHub Desktop. In the Changes tab (on the left panel), you‚Äôll see a list of modified files. Clicking on each file will show a diff view of what was added, deleted, or modified.\nAt the bottom of the Changes tab, you‚Äôll see a text box labeled Summary (required). Enter a descriptive commit message that summarizes the changes you made or the state the repo is in. Again, you want to be able to use these messages to revert the repo back to an older version if necessary.\n\nIf needed, you can add more details about the changes in the ‚ÄúDescription‚Äù field, which appears below the summary box. This is helpful for providing more context about the changes.\n\nBy default, all modified files are selected for the commit. If you don‚Äôt want to commit all the changes, uncheck the files you don‚Äôt want to include in this commit.\nAfter filling in the commit message and selecting the files you want to include, click the ‚ÄúCommit to &lt;branch name&gt;‚Äù button (usually labeled ‚ÄúCommit to main‚Äù or the name of your active branch). This will commit the changes locally.\n\n\n\nPushing\nAfter committing the local changes, click the Push origin button that appears at the top after committing your changes.\n\n\n\nUsing git from the command line\nA final way to use git is from the command line. This is how git was initially inteded to be used, but GUIs have been developed over time.\n\nCloning a repo\nTo clone a repo to your local machine, first, navigate to where you want the repo clone to live.\ncd &lt;path-to-clone-location&gt;\nThen, use the git command clone.\ngit clone &lt;repo-url&gt;\nYou may be asked to provide pass keys or sign in, which may also happen any time you pull or push. To avoid this, set up SSH keys as described here.\n\n\nPulling\nUpon opening a repo, it‚Äôs a good idea to check for changes made to the remote. To do this, you can use\ngit fetch origin\ngit status\nwhich will check for changes and then give you a summary of which files are being tracked, which, have been modified, which are staged, etc. If the local is behind the remote, use\ngit pull\n\n\nCommitting changes\nTo commit changes using the command line, you first need to add modified files to the staging area. To do this, use\ngit add &lt;file_name&gt;\nor\ngit add -A\nto add all modified files.\nNext, commit the modified files using\ngit commit &lt;file_name&gt; -m \"type commit message in quotes after -m flag\"\nor\ngit commit -am \"type commit message here\"\nto commit all changes made to all the staged files using the same message.\n\n\nPushing\nAs you might expect, to push changes up to the remote, you can use\ngit push\nwhich will push all committed changes to GitHub."
  },
  {
    "objectID": "day1.html#footnotes",
    "href": "day1.html#footnotes",
    "title": "Week 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe will not get into using branches much if at all in this workshop.‚Ü©Ô∏é\nI will provide feedback on your homeworks using pull requests.‚Ü©Ô∏é\nWe will not get into using branches much if at all in this workshop.‚Ü©Ô∏é"
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Week 2",
    "section": "",
    "text": "While much more versatile, R can be used as a calculator with some basic syntax.\n\n+ - addition\n- - subtraction\n* - multiplication\n/ - division\n^ - raise to a power. E.g., 2^4 will compute \\(2^4 = 16\\)\n\n\n\n\n\n\n\nYour Turn\n\n\n\nWhat do the %% and %/% operators do? Try experimenting a bit. For example, what‚Äôs the result of\n7 %% 3\nand\n7 %/% 3\n? Repeat with some different values to see if you can figure it out.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n%% - Gives the remainder of a division operation (i.e., ‚Äúx modulo y‚Äù).\n%/% - Gives the integer division of the left hand-side over the right-hand side (i.e., the ‚Äúfloored‚Äù result of \\(x/y\\).)\n\n\n\n\n\n\nOne of the key aspects of coding in R is assignment. That is, assigning objects or values to a name (we will come back to objects shortly).\nAny of =, &lt;-, and -&gt; can act as assignment operators. For example, in\n\na &lt;- 100\nb = 100\n100 -&gt; c\n\nall of a, b, and c are assigned the value 100. We can then use these objects in other computations.\n\n\n\n\n\n\nYour Turn\n\n\n\nCreate a new object named ans that is equal to \\(\\frac{a + b}{c}\\). Now, print a, b, c, and ans by simply typing the names of the objects into the console and pressing ENTER. Did the values of any of the original variables (a, b, and c) change?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNone of the original values change. Code:\nans &lt;- (a + b) / c"
  },
  {
    "objectID": "day2.html#variable-assignment",
    "href": "day2.html#variable-assignment",
    "title": "Week 2",
    "section": "",
    "text": "One of the key aspects of coding in R is assignment. That is, assigning objects or values to a name (we will come back to objects shortly).\nAny of =, &lt;-, and -&gt; can act as assignment operators. For example, in\n\na &lt;- 100\nb = 100\n100 -&gt; c\n\nall of a, b, and c are assigned the value 100. We can then use these objects in other computations.\n\n\n\n\n\n\nYour Turn\n\n\n\nCreate a new object named ans that is equal to \\(\\frac{a + b}{c}\\). Now, print a, b, c, and ans by simply typing the names of the objects into the console and pressing ENTER. Did the values of any of the original variables (a, b, and c) change?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNone of the original values change. Code:\nans &lt;- (a + b) / c"
  },
  {
    "objectID": "day2.html#classes-can-get-elevated",
    "href": "day2.html#classes-can-get-elevated",
    "title": "Week 2",
    "section": "Classes can get ‚Äúelevated‚Äù",
    "text": "Classes can get ‚Äúelevated‚Äù\nClasses may change in R depending on what operation an object or value is involved in. Let‚Äôs explore some scenarios in which this happens.\n\n\n\n\n\n\nYour Turn\n\n\n\nCreate the following two objects in R:\na &lt;- 2L\nb &lt;- 2\n\nWhat class is assigned to each of these objects?\n\nNow, create a third object that is the sum of a and b.\nc &lt;- a + b\n\nWhat class is assigned to c? What happened?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\na has class integer and b has class numeric.\nc has class numeric, meaning that a got elevated to numeric during this operation.\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nLet‚Äôs do another example, one that comes in handy for me all the time. What do you get when you type the following in the console and press enter?\nTRUE + FALSE\nWhat about\nFALSE + FALSE + FALSE\nor\nTRUE + TRUE\n? How are the constants TRUE and FALSE being treated when involved in arithmetic operations?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTRUE is given a value of 1 and FALSE is given a value of 0 when involved in arithmetic operations."
  },
  {
    "objectID": "day2.html#methods-for-classes",
    "href": "day2.html#methods-for-classes",
    "title": "Week 2",
    "section": "Methods for classes",
    "text": "Methods for classes\nClasses of objects have certain methods defined for them. These are generic functions, such as summary() or print(), that have specific instructions for how to handle different classes of objects. In order to see what functions work with a specific class of object, you can use the function methods(). This will give you an idea of what sorts of things you can do with a given object class.\n\n\n\n\n\n\nYour Turn\n\n\n\nType ?methods() into the console and press ENTER in order to learn how to use the function. How many methods are defined for the data.frame class?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are 64 methods defined for the data.frame class."
  },
  {
    "objectID": "day2.html#working-with-vectors",
    "href": "day2.html#working-with-vectors",
    "title": "Week 2",
    "section": "Working with vectors",
    "text": "Working with vectors\nVectors are essentially lists of values. They can be lists of character values, numeric values, logical values, or even more general lists of just about any object type there is. These most general versions of vectors are called lists in R.\n\n\n\n\n\n\nYour Turn\n\n\n\nRun the following code.\n# numeric vector\n(v &lt;- c(1, 2, 3))\n\n# logical vector\n(lv &lt;- v &lt; 2)\n\n# character vector\n(pets &lt;- c(\"dog\", \"cat\", \"bunny\"))\n\n# list is a special kind of vector\n(l &lt;- list(v, lv, pets))\nü§î How did we create the logical vector?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe less-than sign, &lt;, allowed us to ask whether each element of v was less than 2, resulting in a logical vector.\n\n\n\nAccessing elements of a vector is one of the most important things to know. Indexing in R is 1-based, meaning the first element of a vector is considered to be in position 1 (rather than position 0, like many other languages).\n\n\n\n\n\n\nYour Turn\n\n\n\nTo access elements of a list or vector, we use square brackets, []. Try the following:\npets[1]\nlv[3]\nv[c(1,2)]\n# or, the : character can make a range of values\nv[1:3]\nü§î What happens if we make these negative? For example, pets[-1]?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWhen the element number indicators are negative, R returns everything except that index.\n\n\n\nWe can also combine multiple vectors into one. For example, try\n\nw &lt;- c(4, 5, 6)\nvw &lt;- c(v, w)\nvw\n\n[1] 1 2 3 4 5 6"
  },
  {
    "objectID": "day2.html#working-with-dataframes-and-matrices",
    "href": "day2.html#working-with-dataframes-and-matrices",
    "title": "Week 2",
    "section": "Working with dataframes and matrices",
    "text": "Working with dataframes and matrices\nMatrices and dataframes are 2-Dimensional generalizations of vectors, as if stacking vectors on top of each other or side by side. For example, two vectors,\n\\[\n{\\bf v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix},\\ \\ {\\bf w} = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6\\end{bmatrix}\n\\]\ncan be concatenated into a single matrix\n\\[\n{\\bf M} = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}.\n\\]\nIn R, we can also have character and logical matrices.\n\n\n\n\n\n\nYour Turn\n\n\n\nLet‚Äôs get familiar with these common R data structures.\n\nCreate a dataframe in R from scratch.\n\n# combine vectors from above into a dataframe\ndf &lt;- data.frame(\n    v = v,\n    w = w\n)\n\n# View the dataframe\nView(df)\n\n# convert to a matrix\nM &lt;- as.matrix(df)\nM\nNote how we can change the class of df and convert it into a matrix.\n\nLet‚Äôs create a new dataframe based on some different vectors.\n\n# add pets vector from above to a new dataframe\ndf2 &lt;- data.frame(\n    pet = pets,\n    number = c(1, 0, 0)\n)\ndf2\n\n# now convert into a matrix\nmat2 &lt;- as.matrix(df2)\nmat2\nü§î What happened when you converted the second dataframe into a matrix? What does this tell you about dataframes versus matrices?\n\nHint: Use class(df2$number) and class(mat2[, 2]).\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nUpon converting a dataframe with numeric and character columns to a matrix, the numeric columns get elevated to a character so that all columns in the matrix are characters. This tells us that dataframes are more flexible than matrices and can store multiple data types in different columns.\n\n\n\n\nAccessing elements, columns, and rows\nAgain being able to access, add, or remove elements (or even whole columns/rows) from data structures is one of the most important skills to learn. Try the following:\n\n\n\n\n\n\nYour Turn\n\n\n\nmat2\nmat2[1,2]\nmat2[1, ]\nmat2[, 2]\nmat2[, \"pet\"]\nmat2[, \"number\"]\nü§î Why do we need the comma when accessing components of a matrix? What goes before the comma and what goes after?\ndf2\ndf2[1,2]\ndf2[1, ]\ndf2[, 2]\ndf2$number\ndf2$pet\nAs you can see, there are multiple ways to access different components of dataframes and matrices.\nü§î Does the $ syntax work for matrices with named columns, like mat2?"
  },
  {
    "objectID": "day2.html#missing-values",
    "href": "day2.html#missing-values",
    "title": "Week 2",
    "section": "Missing values",
    "text": "Missing values\nYou will almost certainly have some missing data in your own graduate studies, either due to faulty instrumentation, a lapse in focus, or one of many other reasons. That is normal. So normal, that R uses a constant (a special object) NA to encode missing data. It‚Äôs relatively easy to convert other codes for missing data (e.g., 999 or a blank cell in an excel spreadsheet) to NA values, once data are loaded into R or during the loading process. There are even methods designed to identify missing values. For example, try:\n\nis.na(c(2, 3, NA, 5, 6))\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nNA values can have unintended consequences depending on what functions you are using in your analysis code. For example, try the following:\na &lt;- c(2, 3, NA, 5, 6)\nsum(a)\nWhat do you get? What if you wanted to sum up all the values that are present and not missing? Type ?sum(). What does the function documentation say to do to accomplish this?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo get the total of the non-missing values, we can modify the code above to read sum(a, na.rm = TRUE).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the functions mean() and sd(), which take the mean and standard deviation of a vector of values, respectively, behave similarly."
  },
  {
    "objectID": "day2.html#double-versus-single-quotes",
    "href": "day2.html#double-versus-single-quotes",
    "title": "Week 2",
    "section": "Double versus single quotes",
    "text": "Double versus single quotes\nMost of the time, you can use double or single quotes interchangeably.\n\nstr1 &lt;- \"This is a string.\"\nstr2 &lt;- 'This is a string.'\n\n# are they the same?\nstr1 == str2\n\n[1] TRUE\n\n\nHowever, if you want to include quotes inside the string, you need to use single quotes.\n\nstr3 &lt;- 'This is a \"string\".'\nstr3\n\n[1] \"This is a \\\"string\\\".\"\n\n\nNote that printing a string, as we did above with str3, is different from writing the string itself. To do that, we want\n\nwriteLines(str3)\n\nThis is a \"string\".\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nü§î How did R internally modify our string str3 above?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nR added escape characters, \\ to the string to escape the special characters."
  },
  {
    "objectID": "day2.html#special-characters-and-escapes",
    "href": "day2.html#special-characters-and-escapes",
    "title": "Week 2",
    "section": "Special characters and escapes",
    "text": "Special characters and escapes\nThere are a handful of special characters, including things like \\, ', and \". Other special characters you are likely to use are \\n and \\t, which are newline and tab characters, respectively. For example:\n\nstr4 &lt;- \"Using\\nnewline\\ncharacters\"\nwriteLines(str4)\n\nUsing\nnewline\ncharacters\n\n\nTo escape these special characters and use them literally in your string, precede them with \\.\n\nwriteLines(\"\\'Escape\\'\")\n\n'Escape'\n\nwriteLines(\"\\\"Escape\\\"\")\n\n\"Escape\"\n\nwriteLines(\"Some LaTeX: $\\\\bar{x} = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i$\")\n\nSome LaTeX: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$\n\n\nWe can also combine strings with other R output. For example:\n\napprox_pi &lt;- round(pi, digits = 2)\n\nwriteLines(\n    paste0(\"I ate some \", approx_pi, \".\\n\")\n)\n\nI ate some 3.14.\n\n# using paste() with a separator between the strings\nwriteLines(\n    paste(\"Lions\", \"Tigers\", \"and Bears\", \"oh my!\", sep = \", \")\n)\n\nLions, Tigers, and Bears, oh my!"
  },
  {
    "objectID": "day4.html",
    "href": "day4.html",
    "title": "Week 4",
    "section": "",
    "text": "One of R‚Äôs major strengths is in data visualization (i.e., plotting data). The most widely used data visualization package in the R community is ggplot2, which is part of tidyverse. Today‚Äôs lab is all about data visualization."
  },
  {
    "objectID": "day4.html#the-grammar-of-plotting",
    "href": "day4.html#the-grammar-of-plotting",
    "title": "Week 4",
    "section": "The grammar of plotting",
    "text": "The grammar of plotting\nHadley Wickham (2009) is the author of ggplot2 and designed the package to follow The Grammar of Graphics by Wilkinson (2005).\n\nIn brief, the grammar tells us that a graphic maps the data to the aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). Wickham (2015).\n\nThere is some vocabulary to go over.\n(Your notes below)\n\nAesthetics:\nGeometric elements (or geoms):\nLayers:\nScales:\nFacets:\nCoordinate systems:"
  },
  {
    "objectID": "day4.html#ggplot2-syntax",
    "href": "day4.html#ggplot2-syntax",
    "title": "Week 4",
    "section": "ggplot2 syntax",
    "text": "ggplot2 syntax\n\nData are mapped to aesthetics using the aes() function. Aesthetics that can be specified using aes() include:\n\nPosition arguments: x, y, xmin, xmax, etc,\nColor arguments: colour, fill, alpha\nShape and size arguments: shape, size, linetype, linewidth\nGrouping argument: group\n\nGeometric elements are added through the geom_ family of functions. Examples include (but see a full list here):\n\ngeom_point()\ngeom_line()\ngeom_boxplot()\n\nLayers are added by placing as plus sign + between geometric elements.\nFacetting can be done using the facet_wrap() and facet_grid() functions.\nScales can be changes using the family of scale_ functions, including\n\nscale_y_continuous()\nscale_color_manual()\n\nCoordinate systems can be changed using the coord_ family of functions, including\n\ncoord_cartesian()\ncoord_polar()\ncoord_flip()\n\nFinally, themes can be adjusted for the overall appearance of the graphic using theme() and its many (many) arguments."
  },
  {
    "objectID": "day4.html#a-simple-scatter-plot",
    "href": "day4.html#a-simple-scatter-plot",
    "title": "Week 4",
    "section": "A simple scatter plot",
    "text": "A simple scatter plot\nWe can start simple with these data and create a x-y scatterplot to investigate the relationship between variables of interest.\nThe first step is to establish a default ggplot object. This is done through the ggplot() function and letting it know where the data will be coming from.\n\nna_v_k &lt;- ggplot(data = luq_streamchem)\n\nNow, let‚Äôs add a layer. Recall that each layer is made up of aesthetic mappings, mapping data to aesthetics, each of which has its own scale, a geometric element, and any statistical transformations that need to be performed on the data prior to the mapping. This information is supplied through the geom_ family of functions, in combination with aes().\n\nna_v_k &lt;- na_v_k + \n  geom_point(aes(x = na, y = k))\n\nna_v_k\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAlternatively, we can combine these two steps into one with\n\nna_v_k &lt;- ggplot(data = luq_streamchem, aes(na, k)) +\n  geom_point()\n\nna_v_k\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn the above code block, we specify a global aesthetic mapping by putting the aes() call inside the ggplot() function rather than specific to a layer. This allows us to add additional layers, all using the same aesthetic mappings. For example,\n\nna_v_k + geom_point(colour = \"white\", size = 0.75)\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how we can set fixed aesthetics inside the geom_ function without needing to wrap them in aes(). If we want these aesthetics to encode information from the data, we need to map data to the aesthetics using aes(). We will see many examples of this soon."
  },
  {
    "objectID": "day4.html#figures-for-distributions",
    "href": "day4.html#figures-for-distributions",
    "title": "Week 4",
    "section": "Figures for distributions",
    "text": "Figures for distributions\nAnother useful type of plot is a histogram or density plot (a smoothed version of a histogram). Histograms are easy to plot in base R, but let‚Äôs also see how to plot them in using ggplot2(). While ggplot2() requires more typing to create a histogram, it makes creating a density plot easier than with base R (in my opinion).\n\nggplot(data = luq_streamchem, aes(x = na)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote how we don‚Äôt need a y position aesthetic for a histogram, since that is actually implied by the statistical transformation component of the layer, which specifies that we want the count of the number of observations that fall within a given bin.\n\n\nWe can create a density plot in a similar fashion.\n\nggplot(data = luq_streamchem, aes(x = na)) +\n  geom_density()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nLet‚Äôs customize this just a bit in order to get an introduction into how to change the aesthetic mappings to something you find pleasing to the eye.\n\nggplot(data = luq_streamchem, aes(x = na)) +\n  geom_density(fill = \"brown\", color = \"darkgrey\", alpha = 0.8)\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nTry changing the fill, color, and alpha aesthetics for a histogram as well.\nü§î What is the difference between fill and color for a these distribution plots?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfill changes the color of the middle of the geometry while color changes the outline of the geometry."
  },
  {
    "objectID": "day4.html#figures-comparing-groups",
    "href": "day4.html#figures-comparing-groups",
    "title": "Week 4",
    "section": "Figures comparing groups",
    "text": "Figures comparing groups\nLet‚Äôs compare some groups of sugar maple seedlings in Hubbard Brook Experimental Forest, some growing in sites treated with supplemental calcium, others growing in reference sites. Sites were sampled in two years, 2003 and 2004.\n\n# first, load the data\ndata(hbr_maples) \n\n\nBoxplots\nLet‚Äôs first use a box-plot to investigate seedling height as it varies by treatment group using a boxplot.\n\nggplot(data = hbr_maples, aes(x = watershed, y = stem_length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nHere, we are comparing all seedlings from each watershed type from each year, but it might be interesting to see if there is year-to-year variation as well.\n\nü§î How might we ‚Äúadd‚Äù year to the plot such that we can compare the distribution of stem lengths across treatment types and year in the same plot?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOne option would be to map year to a color or fill aesthetic. Another might be to facet by year.\n\n\n\n\n# the first thing we should do is check that `year` is treated\n# as a categorical variable\nhbr_maples %&gt;% pull(year) %&gt;% class()\n\n[1] \"numeric\"\n\n# convert to a factor\nhbr_maples &lt;- hbr_maples %&gt;% mutate(\n  year_f = as.factor(year)\n)\n\nAbove we converted year to a factor because we noticed it was loaded as numeric, but let‚Äôs just see the type of warning message you will encounter if you forget to do something like this in the future.\n\n# example supplying numeric to a grouping factor\nggplot(data = hbr_maples, aes(watershed, stem_length, color = year)) +\n  geom_boxplot()\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n# correctly supplying a factor\nsl_box &lt;- ggplot(data = hbr_maples, aes(watershed, stem_length, color = year_f)) +\n  geom_boxplot()\nsl_box\n\n\n\n\n\n\n\n\nLet‚Äôs add another layer to this plot to show where the actual data fall along the y-axis. This can be accomplished with geom_jitter(), which jitters points along the x-axis so that they don‚Äôt overlap in one line up and down the y-axis.\n\nsl_box +\n  geom_point(position = position_jitterdodge())\n\n\n\n\n\n\n\n\nThe above plot gets into adjusting the position aesthetic. Under the hood, geom_boxplot() used position_dodge() so that the boxplots for two years but the same treatment were not overlapping one another. We needed to match this position aesthetic when we added points to the plot, but we also wanted to add jitter to the position to so that the points are not all on top of one another.\n\n\nDensity plots\nWe can also use density plot to compare distributions of the two treatment groups.\n\nggplot(hbr_maples, aes(x = stem_length, fill = watershed)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nü§î How might we also add the year information to this plot?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOne option in this case would be to facet by either year or watershed.\n\n\n\n\nggplot(hbr_maples, aes(x = stem_length, fill = watershed)) +\n  facet_wrap(vars(year), ncol = 2) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nPlots with errorbars\nA commonly used plot in the literature is called a pointrange plot in ggplot2(). For these plots, we want to plot a point estimate for each group (e.g., the mean) as well as error bars around that point. First, we need to do some data wrangling. Let‚Äôs summarize the data by treatment and year groups and compute the mean and standard error of stem length for each group.\n\n# first get some summaries\nhbr_sl_sum &lt;- hbr_maples %&gt;% \n  group_by(year_f, watershed) %&gt;%\n  summarise(\n    mean = mean(stem_length),\n    se = sd(stem_length) / sqrt(n()),\n    .groups = \"drop\"\n  )\nhbr_sl_sum\n\n# A tibble: 4 √ó 4\n  year_f watershed  mean    se\n  &lt;fct&gt;  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2003   Reference  81.0  1.27\n2 2003   W1         87.9  1.31\n3 2004   Reference  85.9  2.03\n4 2004   W1         97.5  1.79\n\n# now create the plot\nggplot(hbr_sl_sum) +\n  geom_pointrange(\n    aes(\n      x = watershed,\n      y = mean,\n      ymin = mean - se,\n      ymax = mean + se,\n      color = year_f\n    ),\n    position = position_dodge(width = 0.2)\n  )\n\n\n\n\n\n\n\n\nAlternatively, we could do a barplot with error bars.\n\n\n\n\n\n\nNote\n\n\n\nNote that geom_bar() counts the number of observations within a given category by default, while geom_col() allows you to map data to the height of a column.\n\n\n\n# set global aesthetics\nggplot(hbr_sl_sum, aes(x = watershed, y = mean, fill = year_f)) +\n  # add column layer\n  geom_col(\n    position = position_dodge(width = 0.5),\n    width = 0.3\n  ) +\n  # add some error bars\n  geom_errorbar(\n    aes(ymin = mean - se, ymax = mean + se),\n    position = position_dodge(width = 0.5),\n    # this width adjusts the horizontal lines on the errorbars\n    width = 0.1\n  )\n\n\n\n\n\n\n\n\nAs an alternative, we could add a statistical transformation to a bar layer where we override the default statistic and change it to be the mean. We then add a summary statistic layer where we specify the geometry as errorbar and use a ggplot2 function called mean_se() to compute the mean plus or minus one standard error.\n\nggplot(hbr_maples, aes(x = watershed, y = stem_length, fill = year_f)) +\n  # geom bar with mean as the fun argument\n  geom_bar(\n    stat = \"summary\", \n    fun = mean, \n    width = 0.3,\n    position = position_dodge(width = 0.5)\n  ) +\n  # add a statistical summary layer\n  stat_summary(\n    geom = \"errorbar\",\n    fun.data = mean_se,\n    position = position_dodge(width = 0.5),\n    width = 0.1\n  )"
  },
  {
    "objectID": "day4.html#line-plots-and-time-series",
    "href": "day4.html#line-plots-and-time-series",
    "title": "Week 4",
    "section": "Line plots and time series",
    "text": "Line plots and time series\nLet‚Äôs first continue working with the maple seedling data and investigate the relationship between seedling height (stem_length) and leaf area and whether that differs by treatment type.\n\nggplot(\n  hbr_maples, \n  aes(stem_length, corrected_leaf_area, color = watershed)\n) +\n  geom_point()\n\nWarning: Removed 119 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nPerhaps we should also keep with our approach of looking at variation across years as well as treatments.\n\n\n\n\n\n\nYour Turn\n\n\n\n:thinking_face: Provide two aesthetics we could map year to in order to display another axis of variation on the above scatter plot.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOne option would be to use different shapes for the points from different years. Another option would be to facet by year.\n\n\n\n\nggplot(\n  hbr_maples, \n  aes(stem_length, leaf_dry_mass, color = watershed)\n) +\n  geom_point(aes(shape = year_f))\n\n\n\n\n\n\n\n\nWe can apply transformations to data inside ggplot() calls. To illustrate this and to stabilize the mean-variance relationship between these two variables, let‚Äôs log-transform the leaf_dry_mass measurements.\n\nggplot(\n  hbr_maples, \n  aes(stem_length, log(leaf_dry_mass), color = watershed)\n) +\n  geom_point(aes(shape = year_f))\n\n\n\n\n\n\n\n\nLet‚Äôs also see how to add trend lines and smooths to the figure. This can be accomplished with the geom_smooth() function. The default uses a loess smoother, but a simple trend line looks appropriate to me here. To do that, we can set method = \"lm\".\n\nggplot(\n  hbr_maples, \n  aes(stem_length, log(leaf_dry_mass), color = watershed)\n) +\n  geom_point(aes(shape = year_f)) +\n  # add a layer for a smoother\n  geom_smooth(aes(linetype = year_f), method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nAn important note here is that, while we get what look to be confidence bands with these figures, we should almost never use these in a publication graphic. These confidence bands come with a number of standard assumptions, but we have not checked them! It is always safer to provide your own confidence bands that come from rigorous model fitting and checking. This can be done with geom_ribbon(). So, let‚Äôs remove those from the plot.\n\n\nggplot(\n  hbr_maples, \n  aes(stem_length, log(leaf_dry_mass), color = watershed)\n) +\n  geom_point(aes(shape = year_f)) +\n  # add a layer for a smoother\n  geom_smooth(aes(linetype = year_f), method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\ndata(\"and_vertebrates\")\n\nAs a final example, let‚Äôs plot the number of cutthroat trout caught in different reaches of Mack Creek in the H. J. Andrews Experimental forest from 1987 to 2019.\n\nand_counts &lt;- and_vertebrates %&gt;%\n  group_by(year, sitecode, species) %&gt;%\n  summarise(\n    count = n(),\n    .groups = \"drop\"\n  ) %&gt;% \n  drop_na()\n\n\nverts_ts &lt;- filter(and_counts, species != \"Cascade torrent salamander\") %&gt;%\nggplot(., aes(x = year, y = count, color = species)) +\n  facet_wrap(vars(sitecode), ncol = 1) +\n  geom_line() +\n  geom_point()\n\nverts_ts"
  },
  {
    "objectID": "day4.html#customizing-the-appearance",
    "href": "day4.html#customizing-the-appearance",
    "title": "Week 4",
    "section": "Customizing the appearance",
    "text": "Customizing the appearance\nTo change the general appearance of the figures (background colors, grid lines, legend, etc.), we can change the theme. There are a handful of pre-made themes available, which you can view using ?ggplot2::theme_grey(), which is the default. Out of these, I prefer theme_classic() or theme_bw() as a starting point. You can then adjust any of the them elements inside the theme() function.\n\n(verts_ts &lt;- verts_ts + theme_bw())\n\n\n\n\n\n\n\n\nThat looks better than the grey background in my opinion, but what if we didn‚Äôt want the gridlines in the background? This is something we can change with theme(). For a full list of elements you can modify with theme(), see the documentation with ?ggplot2::theme().\n\n# Remove minor gridlines\n(verts_ts &lt;- verts_ts +\n  theme(\n    panel.grid.minor = element_blank()\n  ))\n\n\n\n\n\n\n\n\nLet‚Äôs also make the major vertical gridlines a bit darker.\n\n(verts_ts &lt;- verts_ts + \n   theme(\n     panel.grid.major.x = element_line(color = \"darkgrey\")\n   ))\n\n\n\n\n\n\n\n\nWe can also adjust the background color of the facet labels with theme().\n\n(verts_ts &lt;- verts_ts + \n   theme(\n     strip.background = element_rect(fill = \"#D2AD7CFF\")\n   ))\n\n\n\n\n\n\n\n\n\nAdjusting scales\nWhile aesthetics that are not related to data can be adjusted with theme(), if data are mapped to an aesthetic, we have to change the scale for that aesthetic in order to change its appearance. For example,\n\n(verts_ts &lt;- verts_ts + \n   scale_color_manual(values = c(\"#BE7245FF\", \"#1C3333FF\")))\n\n\n\n\n\n\n\n\nFinally, the y-axis text is a bit scrunched, so let‚Äôs change the breaks there using a scale_ function.\n\n(verts_ts &lt;- verts_ts + \n   scale_y_continuous(breaks = c(0, 100, 200)))\n\n\n\n\n\n\n\n\n\n\nThings to think about when customizing figures\n\nColor blindness: Are your colors color-blind friendly?\nBusiness/ease of interpretation: While figures with lots going on can be pretty to look at, is a busy figure the best way to convey the information you want readers to understand?\n\n\n\nHelpful resources\n\nGeometry explorer: https://www.data-to-viz.com/\nColor palette packages: paletteer\nColor palette explorer: https://r-graph-gallery.com/color-palette-finder"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CoF-intro2R-workshop",
    "section": "",
    "text": "This site hosts lab materials for an introductory R course I designed for incoming graduate students in the College of Forestry at Oregon State University. Feel free to use any of the material that suits your educational needs.\nFor more information on my teaching, research, and consulting, check out my website."
  },
  {
    "objectID": "day5.html",
    "href": "day5.html",
    "title": "Week 5",
    "section": "",
    "text": "Up to this point, we have focused on using the functions defined in the tidyverse family of packages, as well as those defined in base R. However, there will likely come a time when you want or need to do something in R for which you cannot find a package that has pre-built functions to accomplish your goal. You will therefore need to do a little programming, which may include writing your own functions. This lab is meant to introduce you to some useful concepts and functions to help you accomplish custom tasks."
  },
  {
    "objectID": "day5.html#the-initial-conditions",
    "href": "day5.html#the-initial-conditions",
    "title": "Week 5",
    "section": "The initial conditions",
    "text": "The initial conditions\nFirst, we need a starting point, and we need to specify values for paramters \\(\\lambda\\) and \\(\\alpha\\). Let‚Äôs specify some arbitrary parameter values based on known limits for a stable population.\n\nlibrary(tidyverse)\n\nset.seed(55688)\nalpha &lt;- 0.002\nlambda &lt;- 1.5\nsigma &lt;- 0.1\n\n# specify initial population density\nN0 &lt;- 10\n\nOkay, given the initial state of the population, \\(N_0 = 10\\), as well as some arbitrary parameter values, let‚Äôs simulate the population density at time \\(t=1\\) using Equation¬†1.\n\n(N1 &lt;- N0 * lambda * exp(-alpha * N0 + rnorm(1, sd = sigma)))\n\n[1] 14.51858\n\n\nCool, now given \\(N_1\\), we can again simulate the population density for \\(N_2\\).\n\n(N2 &lt;- N1 * lambda * exp(-alpha * N1 + rnorm(1, sd = sigma)))\n\n[1] 21.44161\n\n\nOkay, this is pretty easy, but what if we want to do this for 100 or 1000 time steps? We obviously don‚Äôt want to write 1000 lines of repeating code üò≥."
  },
  {
    "objectID": "day5.html#loops",
    "href": "day5.html#loops",
    "title": "Week 5",
    "section": "Loops üîÅ",
    "text": "Loops üîÅ\nLoops are the bread and butter of many programs, but you will hear advice about avoiding loops in R. This is because loops are slow in R and may get bogged down with complicated loops that run over many iterations. However, most find them more intuitive than our other options in R, so we will start there.\n\nfor() loops\nThe structure of a for() loop in R is\nfor(&lt;counter&gt; in &lt;list&gt;){\n  # code to run each time goes here\n}\nwhere we replace &lt;counter&gt; with the name of an object that changes with each iteration, and &lt;list&gt; with a list or vector that determines how the counter changes over the iterations. For example:\n\n(list_of_values &lt;- 1:4)\n\n[1] 1 2 3 4\n\nfor(i in list_of_values){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n\n\nThe list need not be numeric or consecutive or formatted in any way other than that it must be a vector of some class. For example,\n\noffice_chars &lt;- c(\"Jim\", \"Pam\", \"Dwight\", \"Michael\")\n\nfor(name in office_chars){\n  writeLines(name)\n}\n\nJim\nPam\nDwight\nMichael\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nWork in pairs for 5 minutes to write in words (or pseudo code) how you think the loop should be structured to accomplish simulating a population over 200 time steps using Equation¬†1. What should the list that we iterate over be and what should go inside the curly braces?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n# initialize an empty vector with 200 time steps\nN &lt;- vector(mode = \"double\", length = 200)\n\n# add initial population density\nN[1] &lt;- N0\n\n# loop through the remainder\n# with the vector to loop over as the time steps\nfor(t in 2:200){\n  N[t] &lt;- N[t-1] * lambda * exp(-alpha * N[t-1] + rnorm(1, sd = sigma))\n}\n\n\n\nOkay, let‚Äôs add this to our R script, but let‚Äôs be sure not to hard-code any values into the script. This will allow us to change the simulations more easily later down the line.\n\n# define variable for the number of time steps\nsteps &lt;- 200\n\n# initialize an empty vector with 200 time steps\nN &lt;- vector(mode = \"double\", length = steps)\n\n# add initial population density\nN[1] &lt;- N0\n\n# loop through the remainder\nfor(t in 2:steps){\n  N[t] &lt;- N[t-1] * lambda * exp(-alpha * N[t-1] + rnorm(1, sd = sigma))\n}\n\nggplot(data = data.frame(t = 1:steps, N = N)) +\n  geom_point(aes(x = t, y = N)) +\n  geom_line(aes(x = t, y = N)) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure¬†1: Simulated population density with \\(\\alpha = 0.002,\\ \\lambda = 1.5\\), and \\(\\sigma^2 = 0.01\\).\n\n\n\n\n\nCool!ü§ò\n\n\nwhile() loops\nAnother type of loop is called a while() loop which continues to run while the condition specified inside the while() call evaluates to true.\n\n\n\n\n\n\nWarning\n\n\n\nWhile loops, if not constructed carefully, can result in infinite loops that will never stop running! It‚Äôs important to check your while loops on small test cases before trying to deploy them to bigger jobs so you don‚Äôt end up waiting for something to finish running that will never actually finish.\n\n\nThe structure of a while loop in R is\nwhile(&lt;logical expression&gt;){\n  # do some task that changes the conditions\n  # evaluated in the logical expression\n}\nThis is a bit harder to understand off the bat in my opinion, so here are a couple examples:\n\n# starting conditions\ni &lt;- 1\n\n# set the logical expression to be evaluated\nwhile(i &lt;= 5){\n  # do some operations\n  print(i)\n  # do an operation that changes the conditions\n  # to be evaluated on the next iteration\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\nA short aside into logical expressions\nWe have seen logicals once before when using filter() to subset a dataset. However, there are a few logical operators we have not seen. Here is a list of logical operators:\n\n==: Will evaluate to TRUE if the objects on either side are equal to one another.\n&lt;: Less than will evaluate to TRUE if the numeric value on the left of the operator is less than that on the right.\n&gt;: Greater than will evaluate to TRUE if the numeric value on the left of the operator is greater than that on the right.\n&lt;=: Less than or equal to will evalute to TRUE if the numeric value on the left of the operator is less than or equal to that on the right.\n&gt;=: Greater than or equal to will evaluate to TRUE if the numeric value on the left of the operator is greater than or equal to that on the right.\n!=: Not equal to will evaluate to TRUE if the objects on either side of the operator are not equal.\n\n\nCombining logical expressions using Booleans\nWe can combine multiple logical expressions using Boolean operators, | and &, which are said as or and and, respectively. For example,\n\na &lt;- 4\n\n3 &lt; a & a &lt; 6\n\n[1] TRUE\n\n\nevaluates whether a is between 3 and 6.\n\n\n\n\n\n\nYour Turn\n\n\n\nFirst, simulate a random number between 0 and 10 using num &lt;- runif(1, 0, 10). Working with a partner, take 5 minutes to construct a while loop that finds which two integers from 0, 1, 2, ‚Ä¶, 10 your random number falls between.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n# set initial conditions\nnum &lt;- runif(1, 0, 10)\n\nlower &lt;- 0\nupper &lt;- 1\n\n# start the loop and stop once the answer is found\nwhile(!(lower &lt; num & num &lt; upper)){\n  lower &lt;- lower + 1\n  upper &lt;- upper + 1\n}\n\n# once this stops, we get our answer with\npaste(\"The random number is between\", lower, \"and\", upper) |&gt;\n  writeLines()\n\n\n\n\n\n\nwhile() loop for population growth\nWe could similarly write a while loop to simulate our population growth. The loop in that case would look something like\n\nt &lt;- 2\nwhile(t &lt;= steps){\n  N[t] &lt;- N[t-1] * lambda * exp(-alpha * N[t-1] + rnorm(1, sd = sigma))\n  t &lt;- t + 1\n}\n\nbut we will not change our script in this case to use a while loop. While loops are better for situations in which you don‚Äôt know exactly how many iterations it will take to finish a task (e.g., optimization routines), while for loops are better for cases in which you know the number of steps you will be looping over."
  },
  {
    "objectID": "day5.html#using-the-apply-family-of-functions",
    "href": "day5.html#using-the-apply-family-of-functions",
    "title": "Week 5",
    "section": "Using the apply() family of functions",
    "text": "Using the apply() family of functions\nUsing apply() functions is generally preferred over for() loops in R because they are faster. However, they will not be as useful for simulating population growth because it is difficult to reference previous elements of whatever is being generated. In the population growth example, we use the previously simulated population size, N[t-1], in the calculation of N[t]. This sort of thing is not really what apply() was designed for. However, I will introduce apply() and the tidyverse version, purrr::map(), then demonstrate their use for a different purpose related to the population growth example.\nApply functions work best with existing vectors, matrices, or array. The general syntax is\napply(\n  X =  &lt;array&gt;,\n  MARGIN = &lt;the margin to which we apply the function&gt;,\n  FUN = &lt;some function to apply&gt;\n)\nwhere the X argument is a vector, matrix, or array, FUN is the function you want to apply to some MARGIN of the array. For example,\n\n(X &lt;- matrix(data = 1:6, ncol = 2))\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\napply(X, MARGIN = 2, FUN = mean)\n\n[1] 2 5\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nü§î What did the apply function do? Why did we specify MARGIN = 2? Take a moment to play with the code a bit and discuss with your neighbor.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe code applied the mean() function to the columns of the matrix since rows are margin 1 and columns are margin 2.\n\n\n\nWhen we use apply functions with vectors, we can use some shortcut functions since they are one-dimensional and therefore only have one margin. lapply() applies a function to the elements of a vector of any class and always returns a list. sapply() is similar, but simplifies the results to be a simple vector (a vector of any other class besides a list). For example:\n\n(eg_list &lt;- list(1:3, 4:6, 7:9))\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 4 5 6\n\n[[3]]\n[1] 7 8 9\n\nlapply(eg_list, sum)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 24\n\nsapply(eg_list, sum)\n\n[1]  6 15 24"
  },
  {
    "objectID": "day5.html#using-purrrmap-from-tidyverse",
    "href": "day5.html#using-purrrmap-from-tidyverse",
    "title": "Week 5",
    "section": "Using purrr::map() from tidyverse",
    "text": "Using purrr::map() from tidyverse\nThe tidyverse option for completing repetitive tasks comes from the purrr package with the map() family of functions. The syntax for map() is a bit cleaner than apply() in my opinion (particularly when we get into more complex operations that we want to perform on each element), and is generally structured as\npurrr::map(\n  &lt;vector&gt;, \n  ~ {# do something with each element of vector}\n)\nwhere the &lt;vector&gt; is, like with a for() loop, a vector (often a list) to loop over. The second argument can either be a named function (e.g., sqrt), or can start with a tilde. If it starts with a tilde, everything that comes after gets evaluated during each iteration, just like a for loop. Let‚Äôs see some examples.\n\npurrr::map(eg_list, sum)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 24\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nü§î What class of object is returned by map()? Hint: assign the result of the above map() call to a named object.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmap() always returns a list.\n\n\n\nHere is another example where we use the tilde syntax.\n\n\n\n\n\n\nNote\n\n\n\nNote that .x always takes the place of the current element of the supplied vector in the code that is to be evaluated with each iteration.\n\n\n\npurrr::map(\n  1:5,\n  ~ .x^3\n) %&gt;% unlist()\n\n[1]   1   8  27  64 125\n\n\nNotice how I had to unlist the result of map() above. However, as with sapply(), there are shortcuts that belong to the family of map() functions. For example, if I know the result can be a double(), that is, a numeric vector, then I can use a shortcut:\n\npurrr::map_dbl(\n  1:5,\n  ~ .x^3\n)\n\n[1]   1   8  27  64 125\n\n\nSee ?purrr::map() for the details on the other shortcut functions."
  },
  {
    "objectID": "day5.html#a-simple-grid-search",
    "href": "day5.html#a-simple-grid-search",
    "title": "Week 5",
    "section": "A simple grid search",
    "text": "A simple grid search\nBased on the data, Equation¬†1, and theory of population biology, we can come up with some realistic ranges for possible values of the parameters. First, \\(K = \\frac{\\log(\\lambda)}{\\alpha}\\), where \\(K\\) is the carrying capacity of the population. Second, \\(\\lambda\\) is the density-independent growth factor. This determines the factor by which the population is expected to grow when at low densities. So, if we look at our simulated data,\n\nmean(N[2:5]/N[1:4])\n\n[1] 1.458651\n\n\nmeaning that, when at low densities, the population grew by around 46% (a factor of about 1.46) on average over the first few years. So, perhaps a reasonable range for \\(\\lambda\\) is [1.2, 1.7]. Then, given that \\(K=\\frac{\\log(\\lambda)}{\\alpha}\\), we can solve for \\(\\alpha\\) using a guess of the carrying capacity.\n\n\n\n\n\n\nYour Turn\n\n\n\nBased on Figure¬†1, give an eyeball estimate for \\(K\\) üëÄ.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nI would guess that the carrying capacity is around \\(N = 200\\) since the population density oscillates around 200 or so for the latter part of the time series.\n\n\n\nIf we use our approximations to inform possible ranges of values for \\(\\boldsymbol \\theta\\), we can perform a simple grid search to see which combination of parameter values gives us the best model in terms of minimizing the sum of squared errors.\nLet‚Äôs first establish the grid of parameter values over which we want to evaluate the sum of squared errors. This can be done using the expand.grid() function from base R.\n\n# range for lambda\nlam_range &lt;- seq(1.2, 1.7, length.out = 100)\nalpha_range &lt;- seq(0.001, 0.003, length.out = 100)\n\npar_grid &lt;- expand.grid(lam_range, alpha_range)\n\n# add some column names\ncolnames(par_grid) &lt;- c(\"lambda\", \"alpha\")\n\nhead(par_grid)\n\n    lambda alpha\n1 1.200000 0.001\n2 1.205051 0.001\n3 1.210101 0.001\n4 1.215152 0.001\n5 1.220202 0.001\n6 1.225253 0.001\n\n\nWe now have a each combination of \\(\\alpha\\) and \\(\\lambda\\) in our 100 \\(\\times\\) 100 grid of combinations. The next step will be to write a function that computes the sum of squared errors between the model predictions and the observed data. For this, let‚Äôs start a new R script called R/ssq_ricker.R.\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs generally good practice to make your code modular in that it can stand alone and be applied in other situations. By putting the ssq_ricker() function in its own R script, it is available to me later down the line without needing to run or copy it over from some other analysis script to get it.\n\n\nHere is what our new ssq_ricker.R script should look like:\n\nssq_ricker &lt;- function(y, theta){\n  \n  n &lt;- length(y)\n  preds &lt;- vector(\"double\", length = n)\n  \n  # create the model predictions given the initial population value\n  preds[1] &lt;- y[1]\n  for(t in 2:n){\n    preds[t] &lt;- preds[t-1] * theta[1] * exp(-theta[2] * preds[t-1])\n  }\n  \n  # return the ssq\n  return(\n    sum((y[-1] - preds[-1])^2)\n  )\n  \n}\n\nTo document this function, we can take advantage of some built in tools in RStudio that are there for developers. Put your cursor somewhere (anywhere) inside the function. Then go to Code -&gt; Insert Roxygen skeleton. Your code should then look like this:\n\n#' Title\n#'\n#' @param y \n#' @param theta \n#'\n#' @return\n#' @export\n#'\n#' @examples\nssq_ricker &lt;- function(y, theta){\n  \n  n &lt;- length(y)\n  preds &lt;- vector(\"double\", length = n)\n  \n  # create the model predictions given the initial population value\n  preds[1] &lt;- y[1]\n  for(t in 2:n){\n    preds[t] &lt;- preds[t-1] * theta[1] * exp(-theta[2] * preds[t-1])\n  }\n  \n  # return the ssq\n  return(\n    sum((y[-1] - preds[-1])^2)\n  )\n  \n}\n\n\n\n\n\n\n\nYour Turn\n\n\n\nFill in the Roxygen2 skeleton by adding a title and parameter descriptions after the name of the parameter. For example:\n#' @param y Vector of population densities in chronological order.\n\n\nOnce you finish that, source the function code into the environment. We will then use apply() to find the ‚Äúbest‚Äù combination of parameter values out of our grid.\n\nsource(here::here(\"Lab/Day5/R/ssq_ricker.R\"))\n\nNow, let‚Äôs apply our function to each row of the parameter grid.\n\nssq &lt;- apply(\n  par_grid,\n  MARGIN = 1,\n  FUN = ssq_ricker,\n  y = N\n)\n\nNow we just need to find which row gave the minimum sum of squared errors and we have our parameter estimates!\n\nrow_id_min &lt;- which.min(ssq)\npar_grid[row_id_min, ]\n\n       lambda       alpha\n5364 1.518182 0.002070707\n\n\nThere we go! These are close to what we used to generate the simulated data!\n\n\n\n\n\n\nWarning\n\n\n\nNote that this method really only works because we have data on population growth at low densities, allowing us to estimate \\(\\lambda\\) and therefore \\(\\alpha\\). Otherwise, we can estimate \\(K\\), but not \\(\\lambda\\) and \\(\\alpha\\) (at least with this approach) since \\(K\\) is a function of both \\(\\lambda\\) and \\(\\alpha\\) and we can‚Äôt solve for two unknowns with one equation.\n\n\nLet‚Äôs see a plot of the model overlaid on the observed data. To do this, let‚Äôs first add a new script to the R directory called sim_ricker.R that has a function to generate predictions from the model. This could look like what‚Äôs below, but could be accomplished in many ways.\n\nsim_ricker &lt;- function(theta, N0, steps, stochastic = FALSE, sd = NULL){\n  \n  if(stochastic){\n    if(is.null(sd)){\n      stop(\"When stochastic=TRUE, you must specify a sd for the random errors.\")\n    }\n    N &lt;- vector(\"double\", length = steps)\n    N[1] &lt;- N0\n    for(t in 2:steps){\n      N[t] &lt;- N[t-1] * theta[1] * exp(-theta[2] * N[t-1] + rnorm(1, sd = sd))\n    }\n  } else{\n    N &lt;- vector(\"double\", length = steps)\n    N[1] &lt;- N0\n    for(t in 2:steps){\n      N[t] &lt;- N[t-1] * theta[1] * exp(-theta[2] * N[t-1])\n    }\n  }\n  \n  return(N)\n  \n}\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAdd some documentation to the sim_ricker() function.\n\n\nNote that the code in the sim_ricker() function is similar to what we used in the ssq_ricker() function. Let‚Äôs go ahead and leverage this to clean up the ssq_ricker() function and use sim_ricker() inside of the sum-of-squares calculation.\n\n# edited code for R/ssq_ricker.R\n\nssq_ricker &lt;- function(y, theta){\n  \n  # get model preds\n  n &lt;- length(y)\n  preds &lt;- sim_ricker(theta, N0 = y[1], steps = n)\n  \n  # return the ssq\n  return(\n    sum((y[-1] - preds[-1])^2)\n  )\n  \n}\n\nNow, we can use our knowledge of lapply() to load all the functions we created inside of the Day5/R/ directory.\n\nfuns &lt;- list.files(\n  path = here::here(\"Lab/Day5/R/\"),\n  full.names = T\n)\nlapply(funs, source)\n\nFinally, we can overlay the fitted model on top of the data.\n\n# create dataframe\n# note that we have to convert the theta argument into a \n# double. apply does this internally above when we use\n# the function to find the parameter estimates\ndat &lt;- data.frame(\n  y = N,\n  time = 1:length(N),\n  preds = sim_ricker(\n    as.double(par_grid[row_id_min, ]),\n    N0 = N[1],\n    steps = length(N)\n  )\n)\n\n# plot \nggplot(dat, aes(x = time, y = y)) +\n  geom_point() +\n  geom_line() +\n  geom_line(\n    aes(y = preds),\n    color = \"blue\",\n    linewidth = 1.5\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "Week 3",
    "section": "",
    "text": "Data wrangling (cleaning, formatting, summarizing, and transforming data) may be one of the most time consuming components of any analysis you end up doing. Luckily, the tidyverse family of R packages makes data wrangling easy (or, at least easier).\n\n\n\n\n\n\nYour Turn\n\n\n\n\nCreate a new directory under CoF_intro2R/Lab called Day3.\nGo to the Week 3 module on Canvas and download the three datasets, survey_data.csv, species_info.csv, and location_info.csv to your CoF_intro2R/Lab/Day3/ directory."
  },
  {
    "objectID": "day3.html#transferring-data-to-and-from-disk",
    "href": "day3.html#transferring-data-to-and-from-disk",
    "title": "Week 3",
    "section": "Transferring data to and from disk",
    "text": "Transferring data to and from disk\nreadr includes many tools for reading data into R from disk or writing data from R to disk. They all work pretty similarly, so let‚Äôs get a sense of how they work by exploring the readr::read_csv() function.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nOpen RStudio and open your CoF_intro2R/Lab project.\nRead in the survey_data.csv dataset using readr::read_csv() and relative filepaths. For now, don‚Äôt include any additional arguments to the function.\nUse str() to view the structure of the dataframe.\n\nü§î Do you notice anything odd or incorrect about the way the data were loaded?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a bit undesirable that the Time and Date variables are read in as character variables. This is because the data use periods, ., to encode missing data, which R does not recognize as a missing data character.\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nRead through some of the documentation for read_csv() using ?read_csv().\nü§î How would we adjust our code to fix the unwanted behavior?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can fix the unwanted behavior of reading Time in as a character by adding the argument na = c(\".\", \"\") to the read_csv() call to tell R that both periods and empty cells should be treated as missing data.\n\n\n\nPackage readr also includes functions for writing to disk, which follow a similar structure (e.g., write_csv()). For more on this package, visit the documentation https://readr.tidyverse.org/ or read through the Data import chapter of R for Data Science (Wickham et al.¬†2023)."
  },
  {
    "objectID": "day3.html#formatting-column-names",
    "href": "day3.html#formatting-column-names",
    "title": "Week 3",
    "section": "Formatting column names",
    "text": "Formatting column names\nAfter loading data into R, there will almost always be some formatting required. For example, note the sloppy column names for these datasets. There is a mixture of capitalization styles, sometimes we use spaces, sometimes underscores, etc. Let‚Äôs fix this and use best practices for object names in R.\n\n:camel: or :snake:?\nCurrent best practice in R (and writing code more generally) is to use either camel case :camel: or snake case :snake:. With camel case, each object name starts lower case and, if a more than one word makes up the name, the next words are capitalized. For example:\n# camel case with one word\nname &lt;- object\n\n# camel case with more than one word\nnameOfObject &lt;- object\nSnake case, on the other hand, uses all lower-case letters and replaces spaces between words with underscores (‚Äú_‚Äú). For example:\n# snake case with one word\nname &lt;- object\n\n# snake case with more than one word\nname_of_object &lt;- object\nYou will see that I prefer snake case, but you can pick whichever works best for you.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nRead through the documentation for the dplyr::rename() function using ?dplyr::rename().\nRename the first column of the survey data using rename():\n\nsurvey_data &lt;- rename(\n    survey_data,\n    survey_id = `Survey ID`\n)\n:thinking_face: What do you notice about how we needed to treat the old name inside of the rename() function?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBecause of the space in Survey ID, we had to wrap it in backticks in order for R to treat it as one name.\n\n\n\nThe rename() function is great, but what if we want to rename a bunch of columns at the same time? For example, what if all (or many) of our column names have spaces in them and we want to replace all the spaces with an underscore? Typing out all the new names would be a hassle. Luckily, there is a better way.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nRead the documentation for rename_with() using ?rename_with().\nWe also need to learn a function that can find and replace characters in a string. Read the documentation for stringr::str_replace_all().\nAttempt the following: assign the string ‚ÄúThe Prime Directive‚Äù to a named object in R. Using stringr::str_replace_all(), remove all the spaces in the string.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\npd &lt;- \"The Prime Directive\"\n\n# replace all spaces with nothing\n(pd_nospaces &lt;- str_replace_all(pd, pattern = \" \", replacement = \"_\")\n\n\n\nSo, in order to replace all the spaces in all the names at once, we combine the str_replace_all() function with rename_with().\n\nsurvey_data &lt;- rename_with(\n    survey_data,\n    .fn = stringr::str_replace_all,\n    pattern = \" \",\n    replacement = \"_\"\n)\nnames(survey_data)\n\n[1] \"Survey_ID\"        \"date\"             \"location\"         \"species_observed\"\n[5] \"Time\"             \"Num_observers\"    \"notes\"           \n\n\nFinally, in order for the column names to follow snake case convention :snake:, we now need to convert all the capital letters to lowercase.\n\n\n\n\n\n\nYour Turn\n\n\n\nUse your new knowledge of the rename_with() function and what you learn from ?tolower() to convert the capital letters to lower case in all instances in the column names of survey_data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsurvey_data &lt;- rename_with(\n    survey_data,\n    .fn = tolower\n)\n\n\n\nWe converted the column names to using snake case convention :snake: in two steps above, but let‚Äôs see how to streamline this a bit for next time.\n\nA short aside to introduce ‚Äúpipes‚Äù\nThere are two pipe operators in R, |&gt; and %&gt;%. This first is ‚Äúnative‚Äù to R (at least as of version 4.3.1), while the second comes with tidyverse, but they work in similar ways. Consider the code below:\n\nvec &lt;- c(1:5)\n\n# now find the sum of the squared values greater than 4\nsum(Filter(function(x){x &gt; 4}, sapply(vec, function(x){x^2})))\n\n[1] 50\n\n\nRun this code to verify it works. Now, compare to the following:\n\nsapply(vec, function(x){x^2}) |&gt;\n    Filter(function(x){x &gt; 4}, x = _) |&gt;\n    sum()\n\n[1] 50\n\n\nWhich is easier to read and follow? For most of us, the code with the pipes is much easier to understand.\n\n\n\n\n\n\nNote\n\n\n\nNote the _ character supplied to the x argument of Filter(). The underscore acts as a placeholder for the object that is being piped in from the previous operation. So, when we want to specify which argument we want to assign the piped object to, we use argument = _ for the base R pipes, and argument = . for tidyverse pipes.\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nLoad the two remaining datasets and clean up their column names using what you learned in the two subsections above on pipes and renaming.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsp_info &lt;- read_csv(\"Day3/species_info.csv\", na = c(\".\", \"\")) %&gt;%\n  rename_with(\n    .fn = str_replace_all,\n    pattern = \" \",\n    replacement = \"_\"\n  ) %&gt;%\n  rename_with(\n    .fn = tolower\n  )"
  },
  {
    "objectID": "day3.html#long-versus-wide-data-formats",
    "href": "day3.html#long-versus-wide-data-formats",
    "title": "Week 3",
    "section": "Long versus wide data formats",
    "text": "Long versus wide data formats\nNotice now that, in the survey_data dataframe, the column species_observed includes all the species observed during a given survey in a comma-separated list. Generally, we like to work with data in ‚Äúlong format,‚Äù with one record/observation per row. Let‚Äôs do some wrangling to get these data into long format.\n\n\n\n\n\n\nYour Turn\n\n\n\nü§î Without knowing the exact code, you should still be able to describe the steps you want to take. Work with your neighbor to describe a few steps you could take to reshape these data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nSplit the comma-separated values into separate columns.\nPivot longer, stacking the newly-created columns.\n\n\n\n\nLet‚Äôs now see some code to reshape these data.\nFirst, let‚Äôs split the column into multiple, one for each observed species. The package tidyr has a nice helper function for this called separate_wider_delim(). Check out the documentation for this function.\n\nsurvey_data %&gt;% \n  separate_wider_delim(\n    cols = species_observed,\n    delim = \", \",\n    names_sep = \"_\"\n  )\n\n# A tibble: 4 √ó 8\n  survey_id date    location      species_observed_1   species_observed_2 time  \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;                &lt;chr&gt;              &lt;time&gt;\n1         1 5/10/24 Forest A      Black-tailed Deer    Douglas Squirrel   05:00 \n2         2 5/11/24 Forest B      Northern Spotted Owl Red Fox            18:00 \n3         3 5/12/24 Riverbank     Chinook Salmon       Pacific Tree Frog     NA \n4         4 5/10/24 Mountain Pass Mountain Beaver      Townsend's Mole    10:00 \n# ‚Ñπ 2 more variables: num_observers &lt;dbl&gt;, notes &lt;chr&gt;\n\n\nLet‚Äôs use our new knowledge of pipes and, rather than creating multiple intermediate objects, pipe this result into our next operation: tidyr::pivot_longer().\n\nsurvey_data %&gt;% \n  separate_wider_delim(\n    cols = species_observed,\n    delim = \", \",\n    names_sep = \"_\"\n  ) %&gt;%\n  # next operation is to pivot the two species_observed_\n  # columns to a single column\n  pivot_longer(\n    cols = c(species_observed_1, species_observed_2),\n    names_to = \"record\",\n    values_to = \"species\"\n  )\n\n# A tibble: 8 √ó 8\n  survey_id date    location      time   num_observers notes      record species\n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;time&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  \n1         1 5/10/24 Forest A      05:00              2 Multiple ‚Ä¶ speci‚Ä¶ Black-‚Ä¶\n2         1 5/10/24 Forest A      05:00              2 Multiple ‚Ä¶ speci‚Ä¶ Dougla‚Ä¶\n3         2 5/11/24 Forest B      18:00              3 Owl feath‚Ä¶ speci‚Ä¶ Northe‚Ä¶\n4         2 5/11/24 Forest B      18:00              3 Owl feath‚Ä¶ speci‚Ä¶ Red Fox\n5         3 5/12/24 Riverbank        NA             NA Wetland n‚Ä¶ speci‚Ä¶ Chinoo‚Ä¶\n6         3 5/12/24 Riverbank        NA             NA Wetland n‚Ä¶ speci‚Ä¶ Pacifi‚Ä¶\n7         4 5/10/24 Mountain Pass 10:00              2 &lt;NA&gt;       speci‚Ä¶ Mounta‚Ä¶\n8         4 5/10/24 Mountain Pass 10:00              2 &lt;NA&gt;       speci‚Ä¶ Townse‚Ä¶\n\n\nFinally, we don‚Äôt really need the record column, we just needed to give R somewhere to put the column names. So, let‚Äôs see how to remove a column using the dplyr::select() function.\n\nsurvey_data &lt;- survey_data %&gt;% \n  separate_wider_delim(\n    cols = species_observed,\n    delim = \", \",\n    names_sep = \"_\"\n  ) %&gt;%\n  pivot_longer(\n    cols = c(species_observed_1, species_observed_2),\n    names_to = \"record\",\n    values_to = \"species\"\n  ) %&gt;% \n  select(!record)"
  },
  {
    "objectID": "day3.html#merging-datasets",
    "href": "day3.html#merging-datasets",
    "title": "Week 3",
    "section": "Merging datasets",
    "text": "Merging datasets\nOur next step in preparing the data is to combine all the information into one data frame. To do this, we will use dplyr::left_join(), but you should read the documentation for ?left_join() to see the other types of joins that are possible.\nWe will join the species_info data with the survey_data dataframe first.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nLook through the columns in survey_data and species_info. ü§î Which columns can we use to merge the two datasets?\nü§î How would we pipe this result into another join to add in the location data?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWe can join by the species column, then the location column.\n\nsurvey_data &lt;- left_join(\n    survey_data,\n    sp_info,\n    by = \"species\"\n) %&gt;%\n  left_join(\n      .,\n      loc_info,\n      by = \"location\"\n  )"
  },
  {
    "objectID": "day3.html#adding-and-subsetting-columns",
    "href": "day3.html#adding-and-subsetting-columns",
    "title": "Week 3",
    "section": "Adding and subsetting columns",
    "text": "Adding and subsetting columns\nThere are two key functions in tidyverse used for selecting or adding columns to an existing dataframe or tibble: dplyr::select(), for selecting columns, and dplyr::mutate() for adding columns to a dataframe. select() is relatively easy to use. For example, try:\n\nsurvey_data %&gt;% select(c(date, time))\n\n# A tibble: 8 √ó 2\n  date    time  \n  &lt;chr&gt;   &lt;time&gt;\n1 5/10/24 05:00 \n2 5/10/24 05:00 \n3 5/11/24 18:00 \n4 5/11/24 18:00 \n5 5/12/24    NA \n6 5/12/24    NA \n7 5/10/24 10:00 \n8 5/10/24 10:00 \n\n\nThe c() function can be used to specify a list/vector of columns, but we can also use : to specify a sequence of columns.\n\nsurvey_data %&gt;% select(date:time)\n\n# A tibble: 8 √ó 3\n  date    location      time  \n  &lt;chr&gt;   &lt;chr&gt;         &lt;time&gt;\n1 5/10/24 Forest A      05:00 \n2 5/10/24 Forest A      05:00 \n3 5/11/24 Forest B      18:00 \n4 5/11/24 Forest B      18:00 \n5 5/12/24 Riverbank        NA \n6 5/12/24 Riverbank        NA \n7 5/10/24 Mountain Pass 10:00 \n8 5/10/24 Mountain Pass 10:00 \n\n\nThere are also a handful of useful helper functions. For example,\n\nsurvey_data %&gt;% select(contains(\"_\"))\n\n# A tibble: 8 √ó 5\n  survey_id num_observers scientific_name       conservation_status habitat_type\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;               &lt;chr&gt;       \n1         1             2 Odocoileus hemionus ‚Ä¶ Least Concern       Forest      \n2         1             2 Tamiasciurus douglas‚Ä¶ Least Concern       Forest      \n3         2             3 Strix occidentalis c‚Ä¶ Threatened          Woodland    \n4         2             3 Vulpes vulpes         Least Concern       Woodland    \n5         3            NA Oncorhynchus tshawyt‚Ä¶ Endangered (Some R‚Ä¶ Wetland     \n6         3            NA Pseudacris regilla    Least Concern       Wetland     \n7         4             2 Aplodontia rufa       Species of Concern  Alpine      \n8         4             2 Scapanus townsendii   Least Concern       Alpine      \n\n\nUse ?dplyr::select() to see a full list of helper functions.\nLet‚Äôs also see how to add columns in tidyverse syntax.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nLet‚Äôs add a column for the time spent surveying to the survey data using base R syntax.\n\nsurvey_data$hrs_searched &lt;- c(1.5, 2, 1.75, 1) %&gt;%\n    rep(., each = 2)\n\nNow create a column that is the ‚Äúperson-hours‚Äù, that is, the time spent surveying multiplied by the number of surveyers.\n\n# person-hours\nsurvey_data$person_hrs &lt;- \n    survey_data$hrs_searched * survey_data$num_observers\n\nNow that you have done that the base R way, remove those two new columns, hrs_searched and person_hrs, using dplyr::select().\n\nsurvey_data &lt;- select(\n    survey_data,\n    !c(hrs_searched, person_hrs)\n)\n\nLet‚Äôs add those columns back in using dplyr::mutate().\n\nsurvey_data &lt;- survey_data %&gt;% mutate(\n    hrs_searched = c(1.5, 2, 1.75, 1) %&gt;%\n        rep(., each = 2),\n    person_hrs = num_observers * hrs_searched\n)"
  },
  {
    "objectID": "day3.html#using-the-stringr-package",
    "href": "day3.html#using-the-stringr-package",
    "title": "Week 3",
    "section": "Using the stringr package",
    "text": "Using the stringr package\nWe covered some of the basics of strings last week. This week, we will see what the stringr package has to offer, beyond some of the base R functionality.\n\nMotivating example\nImagine you have data on trees for which the forest type, old growth versus second growth, is ‚Äútied up‚Äù in the tree identifier. For example,\n\n\n\nTree_ID\nDBH\n\n\n\n\ntree_1-old-growth\n50\n\n\ntree_2-old-growth\n46\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\ntree_99-second-growth\n15\n\n\ntree_100-second-growth\n18\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nHow would we extract the tree identifier and separate it from the forest type?\n\n\nRegular expressions\n\n\n\n\n\n\nYour Turn\n\n\n\nSpend 5 minutes skimming the help page on regular expressions with stringr: https://stringr.tidyverse.org/articles/regular-expressions.html.\nü§î How might we ‚Äúmatch‚Äù the substring of interest, tree_, plus one or more numbers?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOne way to do this is:\nstr_extract(tree_id, pattern = \"tree_\\\\d+\")\nAnother way would be:\nstr_split(tree_id, pattern = \"-\")"
  },
  {
    "objectID": "day3.html#working-with-dates",
    "href": "day3.html#working-with-dates",
    "title": "Week 3",
    "section": "Working with dates",
    "text": "Working with dates\nThe final topic we will cover today is working with dates. Dates are common data entries (e.g., sampling date, date of experiment, etc.), but they can also be difficult to work with in R due to the ways dates are commonly formatted. For example, dates are usually combinations of numbers and characters, such as ‚Äú09/30/2024‚Äù or ‚Äú30 Sept.¬†2024‚Äù. Thus, they are usually read into R as character strings, but we might want to treat them as a continuous, numeric variable since each unique data is one day apart. The lubridate package is quite handy for this sort of thing.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nConvert the date column of survey_data to a vector of class date using the function lubridate::mdy().\nConvert the new date column to numeric using as.numeric(). What do you get?\n\nü§î What is the reference date used by lubridate? I.e., what date would you need to pass through as.numeric() in order for R to return 0? &gt; Hint: Try to convert the numeric value from above back into a date using a lubridate function.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nJanuary 1, 1970 is the reference date.\nmdy(\"01/01/1970\") |&gt; as.numeric()"
  }
]